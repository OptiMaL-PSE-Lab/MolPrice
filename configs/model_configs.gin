import gin.torch.external_configurables

include 'configs/dataloader.gin'

# Training Parameters
main.max_epoch = 100
main.early_stopping = True
main.patience = 20
main.no_gpus = 0
main.logging = False
main.gradient_accum = 1

# Model Hyperparameters

# Constants 
optimizer = @torch.optim.Adam
lr = 1e-4
weight_decay=1e-5


# input and count size are max values for the embedding layer
LSTM.input_size = 7000
LSTM.count_size = 100
LSTM.embedding_size = 100
LSTM.hidden_lstm = 100
LSTM.hidden1_nn = 100
LSTM.hidden2_nn = 50
LSTM.output_size = 1
LSTM.lstm_size = 3
LSTM.dropout = 0.1
LSTM.configure_optimizers.optimizer = %optimizer
lstm/torch.optim.Adam.lr = %lr
lstm/torch.optim.Adam.weight_decay = %weight_decay

FP.input_size = %fp_size
FP.hidden_size_1 = 500
FP.hidden_size_2 = 350
FP.hidden_size_3 = 200
FP.dropout = 0.1
FP.configure_optimizers.optimizer = %optimizer
fp/torch.optim.Adam.lr = %lr

# again, transformer input size is a max value for the embedding
lr_tf = 5e-5
Transformer.input_size = 600
Transformer.embedding_size = 320
Transformer.num_heads = 8
Transformer.hidden_size = 600
Transformer.num_layers = 3
Transformer.dropout = 0.1
Transformer.configure_optimizers.optimizer = @transformer/torch.optim.AdamW
transformer/torch.optim.AdamW.lr = %lr_tf
transformer/torch.optim.AdamW.weight_decay = 1e-10
Transformer.configure_optimizers.scheduler = @transformer/torch.optim.lr_scheduler.OneCycleLR
transformer/torch.optim.lr_scheduler.OneCycleLR.max_lr = %lr_tf
transformer/torch.optim.lr_scheduler.OneCycleLR.pct_start = 0.25
# initial guess to be overwritten
transformer/torch.optim.lr_scheduler.OneCycleLR.total_steps = 1e6 