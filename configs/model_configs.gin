import gin.torch.external_configurables

include 'configs/dataloader.gin'

# Training Parameters
main.max_epoch = 100
main.early_stopping = True
main.patience = 10
main.no_gpus = 1
main.logging = True
main.gradient_accum = 2

# Model Hyperparameters

# Constants 
optimizer = @torch.optim.Adam
lr = 5e-5
weight_decay=1e-5


# input and count size are max values for the embedding layer
LSTM.input_size = 7000
LSTM.count_size = 100
LSTM.embedding_size = 100
LSTM.hidden_lstm = 100
LSTM.hidden1_nn = 100
LSTM.hidden2_nn = 50
LSTM.output_size = 1
LSTM.lstm_size = 3
LSTM.dropout = 0.1
LSTM.configure_optimizers.optimizer = %optimizer
lstm/torch.optim.Adam.lr = %lr
lstm/torch.optim.Adam.weight_decay = %weight_decay

FP.input_size = %fp_size
FP.hidden_size_1 = 500
FP.hidden_size_2 = 350
FP.hidden_size_3 = 200
FP.configure_optimizers.optimizer = %optimizer
fp/torch.optim.Adam.lr = %lr

# again, transformer input size is a max value for the embedding
Transformer.input_size = 600
Transformer.embedding_size = 400
Transformer.num_heads = 2
Transformer.hidden_size = 400
Transformer.num_layers = 2
Transformer.dropout = 0.1
Transformer.configure_optimizers.optimizer = @transformer/torch.optim.AdamW
Transformer.configure_optimizers.decay_rate = 0.98
Transformer.configure_optimizers.warmup_epochs = 2
transformer/torch.optim.AdamW.lr = 2e-5
transformer/torch.optim.AdamW.weight_decay = 1e-5

