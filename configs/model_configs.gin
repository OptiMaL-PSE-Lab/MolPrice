import gin.torch.external_configurables

include 'configs/dataloader.gin'

# Training Parameters
bin.train.main.max_epoch = 50
bin.train.main.early_stopping = True
bin.train.main.patience = 20
bin.train.main.no_gpus = 1
bin.train.main.logging = False
bin.train.main.gradient_accum = 4

# Model Hyperparameters

# Constants 
lr = 1e-4
weight_decay=5e-6

lr_tf = 1e-4
decay_tf = 1e-3


# input and count size are max values for the embedding layer
LSTM.input_size = 7000
LSTM.count_size = 100
LSTM.embedding_size = 200
LSTM.hidden_lstm = 200
LSTM.hidden1_nn = 50
LSTM.hidden2_nn = 10
LSTM.output_size = 1
LSTM.lstm_size = 3
LSTM.dropout = 0.1
LSTM.configure_optimizers.optimizer = @lstm/torch.optim.Adam
lstm/torch.optim.Adam.lr = %lr
lstm/torch.optim.Adam.weight_decay = %weight_decay

FP.input_size = %fp_size
FP.hidden_size_1 = 1024
FP.hidden_size_2 = 256
FP.hidden_size_3 = 124
FP.dropout = 0.25
FP.loss_hp = 0.35
FP.loss_sep = %loss_sep
FP.two_d = %two_d
FP.configure_optimizers.optimizer = @fp/torch.optim.Adam
fp/torch.optim.Adam.lr = %lr
fp/torch.optim.Adam.weight_decay = %weight_decay

# again, transformer input size is a max value for the embedding
Transformer.input_size = 500
Transformer.embedding_size = 320
Transformer.num_heads = 4
Transformer.hidden_size = 600
Transformer.num_layers = 3
Transformer.dropout = 0.12
Transformer.configure_optimizers.optimizer = @Transformer/torch.optim.AdamW
Transformer/torch.optim.AdamW.lr = %lr_tf
Transformer/torch.optim.AdamW.weight_decay = %decay_tf
Transformer.configure_optimizers.scheduler = @Transformer/torch.optim.lr_scheduler.OneCycleLR
Transformer/torch.optim.lr_scheduler.OneCycleLR.max_lr = %lr_tf
Transformer/torch.optim.lr_scheduler.OneCycleLR.pct_start = 0.25
# initial guess to be overwritten
Transformer/torch.optim.lr_scheduler.OneCycleLR.total_steps = 1e6 

RoBERTa.hidden_size = 300
RoBERTa.dropout = 0.15
RoBERTa.configure_optimizers.optimizer = @RoBERTa/torch.optim.AdamW
RoBERTa/torch.optim.AdamW.lr = 1e-5
RoBERTa/torch.optim.AdamW.weight_decay = %weight_decay
# initial guess to be overwritten
RoBERTa.configure_optimizers.num_warmup_steps = 1e5
RoBERTa.configure_optimizers.num_training_steps = 5e6